{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "964f78af-89a7-46f2-8e6f-021527227dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aed28713-1c91-4b9f-afde-c23a0439b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data for New York\n",
    "nyc_data = pd.read_csv('NYC_Property_Sales_Apartments.csv')\n",
    "nyc_data = nyc_data[['borough', 'neighborhood', 'gross_square_feet', 'sale_price']]\n",
    "nyc_data['city'] = 'New York'\n",
    "nyc_data['borough'] = nyc_data['borough'].str.lower()\n",
    "nyc_data['neighborhood'] = nyc_data['neighborhood'].str.lower()\n",
    "nyc_data['city'] = nyc_data['city'].str.lower()\n",
    "\n",
    "#Load in data for London\n",
    "london_data = pd.read_csv('London.csv')\n",
    "london_data = london_data.rename(columns={\n",
    "    'City/County': 'city',\n",
    "    'Location': 'neighborhood',\n",
    "    'Area in sq ft': 'gross_square_feet',\n",
    "    'Price': 'sale_price'\n",
    "})\n",
    "london_data['sale_price'] *= 1.24  # adjust the sale price to match USD\n",
    "london_data['borough'] = ''  # since london does not have boroughs we do not need it\n",
    "london_data['city'] = london_data['city'].str.lower()\n",
    "london_data['neighborhood'] = london_data['neighborhood'].str.lower()\n",
    "\n",
    "london_data = london_data[['city', 'borough', 'neighborhood', 'gross_square_feet', 'sale_price']]\n",
    "\n",
    "'''\n",
    "Renamed all of the the columns in the london dataset as well as added a borough column in order to make sure it aligns with the columns that we used for the new york dataset in order\n",
    "for them to be combined and to easily be able to train on both the data sets so that we could use them for evaluation and user input as well. \n",
    "'''\n",
    "\n",
    "#combine the two data sets \n",
    "data = pd.concat([nyc_data, london_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3072b27d-be33-4b1a-99aa-c9ea13d02e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing, filter out potential outliers\n",
    "filtered_data = data[\n",
    "    (data['gross_square_feet'] > 100) & (data['gross_square_feet'] < 20000) &\n",
    "    (data['sale_price'] > 50000) & (data['sale_price'] < 10000000)\n",
    "]\n",
    "\n",
    "features = filtered_data[['city', 'borough', 'neighborhood', 'gross_square_feet']]\n",
    "target = filtered_data['sale_price']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), ['city', 'borough', 'neighborhood'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "'''\n",
    "Column Transformer here is used in order to apply different transformation/preprocessing steps to all of our\n",
    "different columns that we have in our data sets. Additionally this only modifies the columns that are necessary\n",
    "for our particular model and ignores the rest therefore saving some computing time. We also do not put square footage\n",
    "in here since it is already in numerical values while our other features that we want to train on are not, and that\n",
    "is why we have the onehotencoder as well. \n",
    "'''\n",
    "\n",
    "features_encoded = column_transformer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e09b130f-f65e-49ea-94b7-0dc575fcb5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_split=20,\n",
       "                      n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_split=20,\n",
       "                      n_estimators=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', min_samples_split=20,\n",
       "                      n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data sets for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# create our model and fit/train it\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f9c4fd-0849-4b9d-b5af-ff65ce5fee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 658230.83\n",
      "Random Forest R^2: 0.73\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Random Forest RMSE: {rmse:.2f}\")\n",
    "print(f\"Random Forest R^2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93dc316c-02df-49d5-bcd2-8a93ab877436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sale price prediction for the property: $1,749,359.43\n",
      "Sale price prediction for the property: $1,749,359.43\n"
     ]
    }
   ],
   "source": [
    "#allow user input\n",
    "\n",
    "def predict_property_price(city, borough, neighborhood, square_feet):\n",
    "    input_data = {\n",
    "        'city': [city.lower()],\n",
    "        'borough': [borough.lower()],\n",
    "        'neighborhood': [neighborhood.lower()],\n",
    "        'gross_square_feet': [square_feet]\n",
    "    }\n",
    "\n",
    "    input_df = pd.DataFrame(input_data)\n",
    "\n",
    "    transformed_input = column_transformer.transform(input_df)\n",
    "\n",
    "    predicted_price = model.predict(transformed_input)\n",
    "\n",
    "    return f\"Sale price prediction for the property: ${predicted_price[0]:,.2f}\"\n",
    "\n",
    "# change here for user input it is not case sensitive\n",
    "print(predict_property_price('New York', 'Manhattan', 'Tribeca', 800))\n",
    "print(predict_property_price('new york', 'manhattan', 'tribeca', 800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6e2e5-bf04-46ed-8b47-6243adbeab75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
